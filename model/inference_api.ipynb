{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KN_dFKJHTG8"
      },
      "source": [
        "This code below is the one used for interacting between our react interface and our model there's something to change in this code it will be commented but let sum up:\n",
        "\n",
        "*   model name : use your model name and username (huggingface)\n",
        "*   change origin allowed if it's different\n",
        "*   give your token from ngrok (free)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88_qvwEgEfHX"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exYxElpPEpi1"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPCz7p_nFBSz"
      },
      "source": [
        "#**Load your trained and saved model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvTjJMRVEtLl"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "# PUT YOUR USERNAME, NAME THE MODEL\n",
        "model_name = \"username/name_model\"\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLZRltVkFecL"
      },
      "source": [
        "# **Get our template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dUa55RdEwj9"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO_OWnIfFr5V"
      },
      "source": [
        "#**Get our api ready for inference**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdohW3gkEzBM"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "import pickle\n",
        "from pydantic import BaseModel\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "# Add your origin url if it is different\n",
        "origins = [\n",
        "    \"http://localhost:3000\",\n",
        "    \"localhost:3000\"\n",
        "]\n",
        "\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"]\n",
        ")\n",
        "mapping = {\n",
        "    \"role\": \"from\",\n",
        "    \"content\": \"value\",\n",
        "    \"user\": \"human\",\n",
        "    \"assistant\": \"gpt\"\n",
        "}\n",
        "@app.get(\"/\")\n",
        "def index():\n",
        "    return {\"message\" : \"This is the homepage of the API\"}\n",
        "class Request(BaseModel):\n",
        "    input : str\n",
        "@app.post(\"/predict\")\n",
        "def get_response(req: Request):\n",
        "  messages = [\n",
        "    {\"from\": \"human\", \"value\": req.input},]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  assistant_reply = response[0][len(req.input)+19+len(\"<|assistant|>\"):-7]\n",
        "  return {\"messages\" : assistant_reply}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw1v-U4WG69i"
      },
      "source": [
        "Put your token from ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVuHdO1-E2LE"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken \"ur_token\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_iCuZ-vHL0i"
      },
      "source": [
        "#**Running the serve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_oFQKkQE5zG"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "port = 8001\n",
        "ngrok_tunnel = ngrok.connect(port)\n",
        "\n",
        "# where we can visit our fastAPI app\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# finally run the app\n",
        "import uvicorn\n",
        "uvicorn.run(app, port=port)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
